{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Analysis Project\n",
    "\n",
    "* Project Contributors:  Raizel Bernstein and Steven Yan\n",
    "* Project Supervisors:  Fangfang Lee and Justin Tannenbaum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping for Data Collection\n",
    "\n",
    "## Import modules\n",
    "\n",
    "We walk through step-by-step how we went about webscraping through the IMDB website with Beautiful Soup.  We import the following modules for webscraping: requests, time, random, and tqdm.  We import the following modules for data cleaning:  numpy and pandas.  Requests is the module we use for send the http requests to get the HTML files, which we use the module BeautifulSoup to parse.  Numpy is the module we use for certain mathematical operations and array manipulation.  Pandas is the module for organizing the data into a DataFrame, which facilitates data cleaning and analysis.  Time is a module for representing time in code and allows us to create pauses.during code execution, while random helps us to create pseudo-random numbers for the pauses in the code execution.  Tqdm creates the progress bar for executing code for API calling or webscraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping through One Entry\n",
    "\n",
    "We are going to walk through the process of working through a single page before the process of applying the same process to multiple pages.  We start with the URL that we want to scrape.  We make a GET request from the API of the website, and we receive a Response object in return.  We pass the response into the BeautifulSoup constructor, which parses the document with the HTML parser.  We are then able to view the HTML document as a complex tree of Python objects.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = 'https://www.imdb.com/search/title/?groups=top_1000&sort=boxoffice_gross_us,desc'\n",
    "response = requests.get(url)\n",
    "soup = BS(response.text, \"html.parser\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identify the div class of lister-item mode-advanced to contain the information that we need.  We use the find() method to extract the first of the 50 div containers representing the top 50 of the 1000 on the webpage.  We try to put out the value of just one to make sure we have the right values.  We are grabbing movie titles, IMDB id's, PG ratings, the year of release, runtimes, genres, IMDB ratings, metascores, total number of votes submitted for IMDB rating, and gross US earnings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "div = soup.find('div', {'class':'lister-item mode-advanced'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = div.h3.a.text\n",
    "imdb_id = div.find('img')['data-tconst']\n",
    "year = div.h3.find('span', class_='lister-item-year').text\n",
    "pg_rating = div.find('span', class_='certificate').text\n",
    "runtime = div.find('span', class_='runtime').text\n",
    "genre = div.find('span', class_='genre').text\n",
    "imdb_rating = div.strong.text\n",
    "metascore = div.find('span', class_='metascore').text\n",
    "nv = div.find_all('span', attrs={'name': 'nv'})\n",
    "vote = nv[0].text\n",
    "gross = nv[1].text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping through Multiple Entries on Multiple Pages\n",
    "\n",
    "In order to iterate through all 50 instances and then to iterate through all 200 pages for the 1000 entries, we create a for loop to iterate through the 50 movies on each page and another for loop to iterate through each page.  First, we instantiate the variables that will represent each of the data we are gathering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "imdb_ids = []\n",
    "pg_ratings = []\n",
    "years = []\n",
    "runtimes = []\n",
    "genres = []\n",
    "imdb_ratings = []\n",
    "metascores = []\n",
    "votes = []\n",
    "gross_us = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To iterate through each page, we look at the URLs for each page containing 50 entries and find the pattern for iteration.  As we move past the 1st page through the pages, the value after start changes, from 51, 101, 151, 201, etc.  We postulate that if we substitute 1 we would get the original page.  We use string formatting to insert the different values we are iterating through.  We are iterating starting the range of 1 to 1000 at intervals of 50.  As we find the values, we append them to their respective lists.  When we started to run the for loops, we encountered errors.  We added to some of the original code to account for empty values to ensure all of our lists had 1000 values at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Download Progress::   0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29afd879d9294e0a85ded271556d3ee7"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "for n in tqdm(range(1, 1000, 50), desc='Download Progress:'):\n",
    "    # Create an expression that represents each page for the iteration\n",
    "    url = 'https://www.imdb.com/search/title/?groups=top_1000&sort=boxoffice_gross_us,desc&start={}&ref_=adv_nxt'.format(n)\n",
    "    # The requests library makes a get request to the url for data, which is saved to results\n",
    "    results = requests.get(url)\n",
    "    # Create an instance of BeautifulSoup to parse results\n",
    "    soup = BS(results.text, \"html.parser\")\n",
    "    # Find the div container in the HTML that contains the wanted information.  \n",
    "    movie_div = soup.find_all('div', class_='lister-item mode-advanced')\n",
    "    # Use function to suspend execution of calling thread at random\n",
    "    time.sleep(random.randint(3, 12))\n",
    "\n",
    "    # for each container in the div container created above by Beautiful Soup\n",
    "    for div in movie_div:\n",
    "        # to get the titles, we use attribute notation to access the title contained as text in the <a> tag nested inside the <h3> tag \n",
    "        # dot notation only works with the first instance of the tag\n",
    "        title = div.h3.a.text\n",
    "        # we append the scraped title to the titles list through each iteration\n",
    "        titles.append(title)\n",
    "        # to get the IMDB id's, we use the find() method to find the first instance \n",
    "        imdb_id = div.find('img')['data-tconst']\n",
    "        imdb_ids.append(imdb_id)\n",
    "        # to scrape the year, we use the find() method to search nested inside the h3 tag to get the text inside the span tag with the class lister-item-year\n",
    "        year = div.h3.find('span', class_='lister-item-year').text\n",
    "        years.append(year)\n",
    "        # to scrape the pg rating, we use the find method again but create a condition in the case of blank values\n",
    "        pg_rating = div.find('span', class_='certificate').text if div.p.find('span', class_='certificate') else '--'\n",
    "        pg_ratings.append(pg_rating)\n",
    "        # to scrape the runtime, we use a similar method from above\n",
    "        runtime = div.find('span', class_='runtime').text if div.p.find('span', class_='runtime') else '--'\n",
    "        runtimes.append(runtime)\n",
    "        # to scrape the genres, we employed the same method\n",
    "        genre = div.find('span', class_='genre').text\n",
    "        genres.append(genre)\n",
    "        # to scrape the IMDB rating, we call the distinctive strong tag which wraps the desired text\n",
    "        imdb_rating = div.strong.text\n",
    "        imdb_ratings.append(imdb_rating)\n",
    "        # to scrape the etascore, we use a similar code as above\n",
    "        metascore = div.find('span', class_='metascore').text if div.find('span', class_='metascore') else '--'\n",
    "        metascores.append(metascore)\n",
    "        # to scrape the votes and gross us earnings, we use the find_all method, which finds all the instances of the span tag with the name attribute and value of nv\n",
    "        # if there is one item in the list, it represents the vote, and if there are two items, then we get the gross earning and return a string if it's empty to ensure all our lists are the same length to make the DataFrame\n",
    "        nv = div.find_all('span', attrs={'name': 'nv'})\n",
    "        vote = nv[0].text\n",
    "        votes.append(vote)\n",
    "        gross = nv[1].text if len(nv) > 1 else '--'\n",
    "        gross_us.append(gross)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(results, open(\"data/imdb.pickle\", \"wb\" ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrame for Data Cleaning\n",
    "\n",
    "We then created a dataframe that would contain all the values of the lists with each column representing a different type of data.  This allows us to clean and manipulate the data more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas dataframe with the \n",
    "movies = pd.DataFrame({\n",
    "    'movie': titles, 'year' : years, 'pg_rating' : pg_ratings, 'imdb_id' : imdb_ids, 'runtime' : runtimes, 'genre' : genres, 'metascore' : metascores, 'imdb_rating' : imdb_ratings, 'votes' : votes, 'gross_us' : gross_us\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.to_csv(\"data/movies.csv\")"
   ]
  },
  {
   "source": [
    "## Cleaning the Web Scraped Data\n",
    "\n",
    "To determine how the data needs to be cleaned, we called an entry from each columns to see the output. When the values are displayed inside the dataframe, we couldn't determine whether there was whitespace or not. We used df.dtypes to determine whch column values needed to be converted into the appropriate type."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We used extract and regex to pull out any digits of length 1 or more and to turn that string into an integer type\n",
    "movies['imdb_id'] = movies['imdb_id'].str.extract('(\\d+)').astype(int)\n",
    "movies['year'] = movies['year'].str.extract('(\\d+)').astype(int)\n",
    "movies['runtime'] = movies['runtime'].str.extract('(\\d+)').astype(int)\n",
    "# We used rstrip() to remove whitespace right of the metascore\n",
    "movies['metascore'] = movies['metascore'].str.rstrip()\n",
    "# Because of empty values, we can convert those non-numeric values to numeric with coerce\n",
    "movies['metascore'] = pd.to_numeric(movies['metascore'], errors='coerce')\n",
    "# We used replace() to remove the comma from the vote count and turn the value into an integer\n",
    "movies['votes'] = movies['votes'].str.replace(',', '').astype(int)\n",
    "# We used map() and lambda function to apply the lstrip() method and rstrip() method\n",
    "movies['gross_us'] = movies['gross_us'].map(lambda x: x.lstrip('$').rstrip('M'))\n",
    "# We used map() and lambda function to strip \\n in front and whitespace after\n",
    "movies['genre'] = movies['genre'].map(lambda x: x.strip('\\n').rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write dataframe to a CSV file\n",
    "movies.to_csv('data/top_1000_by_us_box_office.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making API requests for Data Collection\n",
    "\n",
    "We collected another data set from TMDB using their sample URL requests tool to create the appropriate URL to find the top 1000 most popular movies.  The code is very similar to the webscraping one, except that we create a request using the Request() constructor to fetch a .json file.  We call the json() method to read and parse the data into a dictionary.  The for loop iterates through each of the pages and appends the data into compiled_list.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/398 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8cacbce5b8c2472392f9f5eb4052125e"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "compiled_list = []\n",
    "\n",
    "for n in tqdm(list(range(1,399))):\n",
    "    url='https://api.themoviedb.org/3/movie/top_rated?api_key=0e72c0b2b11293a6390e9f7b472aec2b&language=en-US&page={}'.format(n)\n",
    "    r = requests.get(url)\n",
    "    data = r.json()\n",
    "    compiled_list.append(data)\n",
    "    time.sleep(random.choice([1,2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(compiled_list, open(\"data/tmdb.pickle\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_2 = []\n",
    "for page in compiled_list:\n",
    "    movies_2 += page['results']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(movies_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['adult', 'backdrop_path', 'genre_ids', 'id', 'original_language',\n",
       "       'original_title', 'overview', 'popularity', 'poster_path',\n",
       "       'release_date', 'title', 'video', 'vote_average', 'vote_count'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['backdrop_path', 'poster_path', 'video', 'adult', 'original_title', 'overview'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon inspecting the data, we realized we had to convert the genre ID's into the actual genre names.  We achieved that by creating dictionary of key:value pairs, where the key is the ID and value is the genre name.  We passed the dictionary through each of the rows of the genre column to replace the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         genre_ids      id original_language  popularity release_date  \\\n",
       "0      [10749, 35]  761053                en      34.985   2020-11-19   \n",
       "1          [10749]  724089                en       6.760   2020-07-31   \n",
       "2  [35, 18, 10749]   19404                hi      14.342   1995-10-20   \n",
       "3          [10749]  696374                en      11.163   2020-05-29   \n",
       "4         [18, 80]     278                en      43.166   1994-09-23   \n",
       "\n",
       "                         title  vote_average  vote_count  \n",
       "0   Gabriel's Inferno Part III           8.9         703  \n",
       "1    Gabriel's Inferno Part II           8.8        1166  \n",
       "2  Dilwale Dulhania Le Jayenge           8.8        2643  \n",
       "3            Gabriel's Inferno           8.7        1931  \n",
       "4     The Shawshank Redemption           8.7       18322  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>genre_ids</th>\n      <th>id</th>\n      <th>original_language</th>\n      <th>popularity</th>\n      <th>release_date</th>\n      <th>title</th>\n      <th>vote_average</th>\n      <th>vote_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[10749, 35]</td>\n      <td>761053</td>\n      <td>en</td>\n      <td>34.985</td>\n      <td>2020-11-19</td>\n      <td>Gabriel's Inferno Part III</td>\n      <td>8.9</td>\n      <td>703</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[10749]</td>\n      <td>724089</td>\n      <td>en</td>\n      <td>6.760</td>\n      <td>2020-07-31</td>\n      <td>Gabriel's Inferno Part II</td>\n      <td>8.8</td>\n      <td>1166</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[35, 18, 10749]</td>\n      <td>19404</td>\n      <td>hi</td>\n      <td>14.342</td>\n      <td>1995-10-20</td>\n      <td>Dilwale Dulhania Le Jayenge</td>\n      <td>8.8</td>\n      <td>2643</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[10749]</td>\n      <td>696374</td>\n      <td>en</td>\n      <td>11.163</td>\n      <td>2020-05-29</td>\n      <td>Gabriel's Inferno</td>\n      <td>8.7</td>\n      <td>1931</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[18, 80]</td>\n      <td>278</td>\n      <td>en</td>\n      <td>43.166</td>\n      <td>1994-09-23</td>\n      <td>The Shawshank Redemption</td>\n      <td>8.7</td>\n      <td>18322</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre = {28: 'Action', 12: \"Adventure\", 16: 'Animation', 35: 'Comedy', 80: 'Crime', 99: 'Documentary', 18: 'Drama', 10751: 'Family', 14: 'Fantasy', 36: 'History', 27: 'Horror', 10402: 'Music', 9648: 'Mystery', 10749: 'Romance', 878: 'Science Fiction', 10770: 'TV Movie', 53: 'Thriller', 10752: 'War', 37: 'Western'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for g_id in df['genre_ids']:\n",
    "    for g in g_id:\n",
    "        if g not in genre.keys():\n",
    "            print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['genre_ids'] = df['genre_ids'].apply(lambda g_id: [genre[g] for g in g_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/tmdb_top_rated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining data from two databases\n",
    "\n",
    "We uploaded the CSV from the webscraping exercise, merged the two dataframes together, and inspected our resulting dataframe, and dropped extraneous columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['genre_ids', 'id', 'original_language', 'popularity', 'release_date',\n",
       "       'title', 'vote_average', 'vote_count', 'movie', 'year', 'pg_rating',\n",
       "       'imdb_id', 'runtime', 'genre', 'metascore', 'imdb_rating', 'votes',\n",
       "       'gross_us'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "top = pd.read_csv('data/top_1000_by_us_box_office.csv')\n",
    "combined = pd.merge(df, top, left_on = 'title', right_on = 'movie')\n",
    "combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined.drop(columns=['movie', 'metascore', 'vote_count', 'vote_average', 'original_language', 'id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We checked for any null values in the table, and dropped the rows which had null values for the gross earnings.  We also wanted to pull the month and the year out of the release date, so we could sort by month or year.  By checking dtypes, we saw that certain columns needed to be converted into integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "genre_ids       0\n",
       "popularity      0\n",
       "release_date    0\n",
       "title           0\n",
       "year            0\n",
       "pg_rating       0\n",
       "imdb_id         0\n",
       "runtime         0\n",
       "genre           0\n",
       "imdb_rating     0\n",
       "votes           0\n",
       "gross_us        0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "combined.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['month'] = combined['release_date'].apply(lambda m: m[5:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['year'] = combined['year'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['month'] = combined['month'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were no null values for the gross_us column, but by using value_counts() we could see that there were rows with the value of '--'.  We subsetted out the rows containing the string, and then converted the column to float values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.gross_us.value_counts()\n",
    "combined = combined[~combined.gross_us.str.contains(\"--\")]\n",
    "combined.gross_us = combined.gross_us.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.to_csv('data/combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}